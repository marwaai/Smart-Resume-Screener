{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pqsXrYLR5Cb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceEmbeddingTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, model_name='all-mpnet-base-v2'):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return self.model.encode(X, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "NYvE5XEZTPQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Validation & Data Strategy\n",
        "1. Integrating Hugging Face Datasets\n",
        "To rigorously test the model's accuracy, we utilized the Resume Screening Dataset from Hugging Face. This provides a diverse benchmark of real-world CVs across multiple domains.\n",
        "\n",
        "2. Refining the Evaluation Logic (Skill-Based vs. Administrative)\n",
        "While the dataset includes labels for \"Accepted\" or \"Rejected\", our system adopts a more objective Semantic Matching approach.\n",
        "\n",
        "The Problem with Labels: Upon analyzing the Reason column in the dataset, many resumes were \"Rejected\" due to factors external to the Job Description (softskills).\n",
        "\n",
        "The Solution: creating a \"Random Baseline\" (Incorrect Data)(more illustration  about before it is cell\n"
      ],
      "metadata": {
        "id": "t6kOXpWkSOy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"hf://datasets/AzharAli05/Resume-Screening-Dataset/dataset.csv\")\n"
      ],
      "metadata": {
        "id": "Bpj3DEg2Th1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # del HTML tags\n",
        "    text = re.sub(r\"<.*?>\", \"\", text)\n",
        "\n",
        "    # del email\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"[EMAIL]\", text)\n",
        "\n",
        "    # del extra spaces\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "\n",
        "    # make  lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    return text.strip()\n"
      ],
      "metadata": {
        "id": "c2q78biGl41b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the dataset has the required columns\n",
        "# Use this assertion to catch data format issues early\n",
        "assert 'Resume' in df.columns and 'Job_Description' in df.columns, \"Missing required columns\"\n"
      ],
      "metadata": {
        "id": "MHfJZYEcOe8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#apply clean on resume col\n",
        "df[\"Resume\"] = df[\"Resume\"].apply(clean_text)"
      ],
      "metadata": {
        "id": "kTRjA0YaS9ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_embeddings(df):\n",
        "\n",
        "    pipeline = Pipeline([('embedder', SentenceEmbeddingTransformer())])\n",
        "\n",
        "    resume_embeddings = pipeline.transform(df['Resume'])\n",
        "\n",
        "    job_embeddings = pipeline.transform(df['Job_Description'])\n",
        "\n",
        "\n",
        "    return resume_embeddings, job_embeddings\n"
      ],
      "metadata": {
        "id": "CfKp_RDhO2FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_embeddings, job_embeddings=generate_embeddings(df)"
      ],
      "metadata": {
        "id": "sDyZ657RZfOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarities = [\n",
        "    cosine_similarity([resume_embeddings[i]], [job_embeddings[i]])[0][0]\n",
        "    for i in range(len(df))]\n",
        "df['match_score'] = np.array(similarities) * 100\n"
      ],
      "metadata": {
        "id": "LWQIgyqDWEhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To verify that the model is truly capturing the semantic relationship between resumes and jobs by creating a \"Random Baseline\" (Incorrect Data).\n",
        "\n",
        "1.  Shuffle Data: We randomize the Resume column so that resumes no longer match their original labels/jobs.\n",
        "2.  Generate Embeddings: We process this shuffled data through the model to get \"Wrong\" embeddings.\n",
        "3.  Validation: By comparing these to the original data, we ensure the model's similarity scores mean for random pairs are significantly lower than for correct pairs. This proves the model isn't giving high scores by chance."
      ],
      "metadata": {
        "id": "UHJKhCvyTgjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_wrong = df.copy()\n",
        "df_wrong['Resume'] = df['Resume'].sample(frac=1).values\n",
        "resume_embeddingsw, job_embeddingsw=generate_embeddings(df_wrong)"
      ],
      "metadata": {
        "id": "_rz9VoGGucrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similaritiesw = [\n",
        "    cosine_similarity([resume_embeddingsw[i]], [job_embeddingsw[i]])[0][0]\n",
        "    for i in range(len(df))]\n",
        "df_wrong['match_score'] = np.array(similaritiesw) * 100\n"
      ],
      "metadata": {
        "id": "3RP9t_UgvzSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_simple_distribution(df,s):\n",
        "    \"\"\"\n",
        "    ÿ±ÿ≥ŸÖ ÿ™Ÿàÿ≤Ÿäÿπ ŸÜÿ≥ÿ® ÿßŸÑŸÖÿ∑ÿßÿ®ŸÇÿ© (Match Scores) ŸÖÿπ ÿ™Ÿàÿ∂Ÿäÿ≠ ÿßŸÑŸÖÿ™Ÿàÿ≥ÿ∑ ŸàÿßŸÑŸàÿ≥Ÿäÿ∑ ŸÅŸÇÿ∑.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # ÿ±ÿ≥ŸÖ ÿßŸÑŸáŸäÿ≥ÿ™Ÿàÿ¨ÿ±ÿßŸÖ ŸÖÿπ ŸÖŸÜÿ≠ŸÜŸâ ÿßŸÑŸÉÿ´ÿßŸÅÿ© (KDE)\n",
        "    sns.histplot(df['match_score'], bins=50, kde=True, color='skyblue', edgecolor='black')\n",
        "\n",
        "    # ÿ•ÿ∂ÿßŸÅÿ© ÿÆÿ∑ ÿßŸÑŸÖÿ™Ÿàÿ≥ÿ∑ (Mean)\n",
        "    mean_val = df['match_score'].mean()\n",
        "    plt.axvline(mean_val, color='green', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}')\n",
        "\n",
        "    # ÿ•ÿ∂ÿßŸÅÿ© ÿÆÿ∑ ÿßŸÑŸàÿ≥Ÿäÿ∑ (Median)\n",
        "    median_val = df['match_score'].median()\n",
        "    plt.axvline(median_val, color='red', linestyle='--', linewidth=2, label=f'Median: {median_val:.2f}')\n",
        "\n",
        "    # ÿ¨ŸÖÿßŸÑŸäÿßÿ™ ÿßŸÑÿ±ÿ≥ŸÖ\n",
        "    plt.title(s)\n",
        "    plt.xlabel(s+' (%)')\n",
        "    plt.ylabel('Count')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "g4isBxJFdxms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "in plot The shift in the distribution mean from 54.23% (Original) to 39.62% (Shuffled) proves that the model successfully distinguishes between relevant and random Resume-Job pairs."
      ],
      "metadata": {
        "id": "qC1Zj606VxmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_simple_distribution(df_wrong,'Distribution of wrong Scores')\n",
        "plot_simple_distribution(df,'Distribution of Match Scores')\n"
      ],
      "metadata": {
        "id": "AwDbAJ9vvzS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test model on real job describtion"
      ],
      "metadata": {
        "id": "99ikcBzf4qRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "\n",
        "def rank_resumes(job_text, resume_df, pipeline, top_n=5):\n",
        "    \"\"\"It sorts all resumes from highest to lowest score and returns the top $N$ candidates.\n",
        "    job_text:job des\n",
        "    resume_df:from data_set\"\"\"\n",
        "    job_embedding = pipeline.transform([job_text])\n",
        "\n",
        "    resume_embeddings = pipeline.transform(resume_df['Resume'])\n",
        "\n",
        "    scores = cosine_similarity(job_embedding, resume_embeddings).flatten()\n",
        "\n",
        "    resume_df['match_score'] = scores * 100\n",
        "\n",
        "    def get_reason(resume_txt):\n",
        "        res_words = set(re.findall(r'\\w+', resume_txt.lower()))\n",
        "        job_words = set(re.findall(r'\\w+', job_text.lower()))\n",
        "        common = [w for w in res_words.intersection(job_words) if len(w) > 3]\n",
        "        return f\"Matches key terms: {', '.join(common[:10])}\"\n",
        "\n",
        "    resume_df['justification'] = resume_df['Resume'].apply(get_reason)\n",
        "\n",
        "    ranked_results = resume_df.sort_values(by='match_score', ascending=False).head(top_n)\n",
        "\n",
        "    return ranked_results[['Resume', 'match_score','justification']]"
      ],
      "metadata": {
        "id": "DluqiiL-WcCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top=rank_resumes(job_text = \" professionals who hire, screen and plan for new employees to create a positive work environment. They consult with executives to .\", resume_df = df, pipeline = Pipeline([('embedder', SentenceEmbeddingTransformer())]), top_n = 5)"
      ],
      "metadata": {
        "id": "QP6c8vRSdHCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in top.values:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "0TJPCZ0sYRCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To download a specialized job-skills dataset and convert it into a structured JSON cause first data set not has skill col\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "path = kagglehub.dataset_download(\"batuhanmutlu/job-skill-set\")\n",
        "\n",
        "import os\n",
        "csv_file = [f for f in os.listdir(path) if f.endswith('.csv')][0]\n",
        "full_path = os.path.join(path, csv_file)\n",
        "\n",
        "df = pd.read_csv(full_path)\n",
        "\n",
        "df_clean = df[['job_title', 'job_skill_set']].dropna()\n",
        "\n",
        "data_to_save = df_clean.to_dict(orient='records')\n",
        "\n",
        "json_file_path = \"job_skills_data.json\"\n",
        "\n",
        "with open(json_file_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(data_to_save, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "\n",
        "print(json.dumps(data_to_save[:2], indent=4, ensure_ascii=False))"
      ],
      "metadata": {
        "id": "keKAo2FPuh7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "start bulding flask app"
      ],
      "metadata": {
        "id": "wJGOWyXzcGzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During model evaluation, it was observed that the Mean Cosine Similarity for correct matches was 54.23%, while random matches averaged 39.62%. From a mathematical perspective, this is a clear distinction. However, from a User Experience (UX) perspective, a user seeing a '54% match' for a high-quality resume might perceive the AI as inaccurate or failing. To address this, I implemented a Square Root scaling function. In my opinion, using the square root is superior to adding fixed points because it inherently prevents the score from exceeding 100%, ensuring the output remains logical and user-friendly."
      ],
      "metadata": {
        "id": "nj09WLr1Y6ND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine Similarity often suffers from Length Bias. When a resume is very long, the core skills and relevant experience can get 'diluted' or lost in the background noise of administrative text,\n",
        "leading to a misleadingly low score. Standard models might struggle to find the 'signal' in a 5-page document.\n",
        "so\n",
        "Instead of processing the entire resume as a single block, I implemented a Sliding Window Chunking strategy. The resume is split into smaller, overlapping segments (Chunks).\n",
        "\n",
        "Maximum Similarity (Max-Pooling): We calculate the similarity for each individual chunk against the job description and take the Maximum Score (torch.max)."
      ],
      "metadata": {
        "id": "VFg4QbipdFua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask flask-ngrok spacy pdfplumber python-docx sentence-transformers -q\n",
        "!python -m spacy download en_core_web_sm -q"
      ],
      "metadata": {
        "id": "O3QAar8Ovml2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import pdfplumber\n",
        "import docx\n",
        "import spacy\n",
        "import ast\n",
        "import math\n",
        "from flask import Flask, request\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from google.colab.output import eval_js\n",
        "app = Flask(__name__)\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\") # for NER\n",
        "except:\n",
        "    os.system(\"python -m spacy download en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_text(file):\n",
        "    fname = file.filename.lower()\n",
        "    if fname.endswith('.pdf'):\n",
        "        with pdfplumber.open(file) as pdf:\n",
        "            return \" \".join([p.extract_text() for p in pdf.pages if p.extract_text()])\n",
        "    elif fname.endswith('.docx'):\n",
        "        doc = docx.Document(file)\n",
        "        return \" \".join([p.text for p in doc.paragraphs])\n",
        "    return file.read().decode('utf-8')\n",
        "\n",
        "def get_person_name(text): #to extract person name\n",
        "    doc = nlp(text[:500])\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"PERSON\":\n",
        "            return ent.text\n",
        "    return \"unknown\"\n",
        "\n",
        "def get_companies(text):#to extract past experince\n",
        "    doc = nlp(text)\n",
        "    companies = set([ent.text.strip() for ent in doc.ents if ent.label_ == \"ORG\"])\n",
        "    return list(companies)[:5]\n",
        "\n",
        "def load_jobs():#the json come from data set\n",
        "    try:\n",
        "        with open('job_skills_data.json', 'r', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "    except:#If the JSON file is missing or corrupted, the app won't crash. Instead, it loads a 'Default Job' (Python Developer) to keep the system functional.\n",
        "        return [{\"job_title\": \"Python Developer\", \"job_description\": \"Experience with Flask\", \"job_skill_set\": \"['Python', 'Flask', 'SQL']\"}]\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return '''\n",
        "    <div style=\"text-align:center; padding:100px; font-family:sans-serif; background:#f4f7f6; height:100vh;\">\n",
        "        <div style=\"background:white; padding:40px; display:inline-block; border-radius:15px; box-shadow:0 4px 15px rgba(0,0,0,0.1);\">\n",
        "            <h2 style=\"color:#2c3e50;\">üß™ Smart CV Scanner </h2>\n",
        "            <p style=\"color:#7f8c8d;\"> Upload your file (PDF/DOCX) to analyze and match it with top job roles</p>\n",
        "            <form action=\"/test\" method=\"post\" enctype=\"multipart/form-data\">\n",
        "                <input type=\"file\" name=\"resume\" required style=\"margin:20px 0;\"><br>\n",
        "                <button type=\"submit\" style=\"padding:12px 30px; background:#27ae60; color:white; border:none; border-radius:5px; cursor:pointer; font-size:16px;\">start now</button>\n",
        "            </form>\n",
        "        </div>\n",
        "    </div>\n",
        "    '''\n",
        "\n",
        "@app.route('/test', methods=['POST'])\n",
        "def test():\n",
        "    if 'resume' not in request.files: return \"no uploaded file\"\n",
        "\n",
        "    file = request.files['resume']\n",
        "    text = extract_text(file)\n",
        "\n",
        "    person_name = get_person_name(text)\n",
        "    detected_companies = get_companies(text)\n",
        "\n",
        "    chunks = [text[i:i+400] for i in range(0, len(text), 250)]\n",
        "    chunk_embeddings = model.encode(chunks, convert_to_tensor=True)\n",
        "\n",
        "    jobs = load_jobs()\n",
        "    job_texts = [j.get('job_description', j['job_title']) for j in jobs]\n",
        "    job_embeddings = model.encode(job_texts, convert_to_tensor=True)\n",
        "\n",
        "    sim_matrix = util.cos_sim(chunk_embeddings, job_embeddings)\n",
        "    raw_score = torch.max(sim_matrix).item()\n",
        "\n",
        "    boosted_score = math.sqrt(max(0, raw_score)) * 100\n",
        "    final_display_score = round(min(boosted_score, 99.9), 1)\n",
        "    #choose biggest num im tensor\n",
        "    flat_idx = torch.argmax(sim_matrix).item()\n",
        "\n",
        "    chunk_idx = flat_idx // len(jobs)\n",
        "    job_idx = flat_idx % len(jobs)\n",
        "    raw_skills = jobs[job_idx].get('job_skill_set', [])\n",
        "    try:\n",
        "        #hadel\"[\"python\",\"java\"]\" list write as str\n",
        "        job_skills_list = ast.literal_eval(raw_skills) if isinstance(raw_skills, str) else raw_skills\n",
        "    except:\n",
        "        job_skills_list = [s.strip() for s in str(raw_skills).split(',')]\n",
        "\n",
        "    skills_html = \"\".join([f'<span style=\"background:#3498db; color:white; border-radius:20px; padding:5px 15px; margin:5px; display:inline-block; font-size:12px;\">{s}</span>' for s in job_skills_list])\n",
        "    companies_html = \", \".join(detected_companies) if detected_companies else \"unknown\"\n",
        "\n",
        "    return f'''\n",
        "    <div style=\"max-width:750px; margin:40px auto; padding:30px; font-family:'Segoe UI', Arial; direction:rtl; line-height:1.8; background:#fff; border-radius:15px; box-shadow:0 10px 30px rgba(0,0,0,0.15);\">\n",
        "        <h2 style=\"color:#2c3e50; text-align:center;\">üìä Result for : {person_name}</h2>\n",
        "\n",
        "        <div style=\"display:flex; justify-content:space-between; align-items:center; background:#f8f9fa; padding:20px; border-radius:12px; margin:25px 0; border-right:5px solid #27ae60;\">\n",
        "            <div>\n",
        "                <span style=\"color:#7f8c8d; font-size:14px;\">  job_title :</span><br>\n",
        "                <strong style=\"font-size:1.4em; color:#2c3e50;\">{jobs[job_idx]['job_title']}</strong>\n",
        "            </div>\n",
        "            <div style=\"text-align:center;\">\n",
        "                <span style=\"color:#7f8c8d; font-size:14px;\">  score </span><br>\n",
        "                <div style=\"font-size:32px; font-weight:bold; color:#27ae60;\">{final_display_score}%</div>\n",
        "            </div>\n",
        "        </div>\n",
        "\n",
        "        <div style=\"margin-bottom:20px;\">\n",
        "            <b style=\"color:#e67e22;\">üë§  name :</b> {person_name}\n",
        "        </div>\n",
        "\n",
        "        <div style=\"margin-bottom:25px;\">\n",
        "            <b style=\"color:#2980b9;\">üè¢  past experince:</b> {companies_html}\n",
        "        </div>\n",
        "\n",
        "        <div style=\"margin-bottom:25px;\">\n",
        "            <b style=\"color:#2c3e50;\">üõ† needed skills:</b><br>\n",
        "            <div style=\"padding-top:10px;\">{skills_html}</div>\n",
        "        </div>\n",
        "\n",
        "        <div style=\"text-align:center; margin-top:40px;\">\n",
        "            <a href=\"/\" style=\"text-decoration:none; background:#2c3e50; color:white; padding:12px 35px; border-radius:8px;\"> new test </a>\n",
        "        </div>\n",
        "    </div>\n",
        "    '''\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        print(\"---\")\n",
        "        print(\"ÿßÿ∂ÿ∫ÿ∑ ÿπŸÑŸâ ÿßŸÑÿ±ÿßÿ®ÿ∑ ÿ®ÿßŸÑÿ£ÿ≥ŸÅŸÑ ŸÑŸÅÿ™ÿ≠ ÿßŸÑŸàÿßÿ¨Ÿáÿ©:\")\n",
        "        print(eval_js(\"google.colab.kernel.proxyPort(5000)\"))\n",
        "        print(\"---\")\n",
        "    except:\n",
        "        pass\n",
        "    app.run(port=5000)"
      ],
      "metadata": {
        "id": "SVbnk9Ywv6Vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dvxqdx11ivnK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}